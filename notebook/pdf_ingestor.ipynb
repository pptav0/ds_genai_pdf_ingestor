{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f2c51f-c080-4c1a-9387-87dcc599ba79",
   "metadata": {},
   "source": [
    "# Project Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac3cea-bc1e-4310-83cd-69186dfb3f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c6ca2-1977-42a1-8a83-626866ccf756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a8ad3-46c5-499a-8ebe-8c619ec06492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47da99bb-a4fc-4e6e-a552-2034c3a68e5a",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24738632-9cdb-46e5-8ca0-dafc7fc38127",
   "metadata": {},
   "source": [
    "## a. Folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "02da2cc0-1f68-4967-833c-6535b413bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Set enviroment varaible\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# Jupyter-specific\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29d31bc-e33c-4681-a718-852dd8bfa3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project structure\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
    "SRC_DIR      = PROJECT_ROOT / \"src\"\n",
    "\n",
    "# Add project src/ directory to PYTHONPATH\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113cec0-45e2-43f5-9e2b-c7733ed6255b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aadb60b7-ee23-4513-91d3-a5859f9d2257",
   "metadata": {},
   "source": [
    "## b. Load Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422ab6ba-66b8-428a-9fb1-8e9b6ba1041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl session var = <curl_cffi.requests.session.Session object at 0x10893a0f0>\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: This section is required to setup a hybrid workflow between Google Colab or local venv\n",
    "import curl_cffi\n",
    "print('curl session var =', curl_cffi.requests.Session(impersonate=\"chrome\"), flush=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "abbe312d-a87d-48c6-9b31-6f5c4dc434a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB = False\n",
      "Python   = 3.12.8\n",
      "Platform = darwin\n"
     ]
    }
   ],
   "source": [
    "# --- Detect Colab vs Local ---\n",
    "def _is_colab() -> bool:\n",
    "    \"\"\"Return True when running inside Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "IN_COLAB = _is_colab()\n",
    "\n",
    "print(\"IN_COLAB =\", IN_COLAB)\n",
    "print(\"Python   =\", sys.version.split()[0])\n",
    "print(\"Platform =\", sys.platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2effdd1d-4062-42a8-8e1f-61a82b831f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mount Drive first (Colab only) ---\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    COLAB_DRIVE_MOUNT = \"/content/drive\"\n",
    "    drive.mount(COLAB_DRIVE_MOUNT, force_remount=True)\n",
    "\n",
    "    os.chdir(f\"{COLAB_DRIVE_MOUNT}/{NOTEBOOK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3e8f7c8a-e351-4893-ab15-5196fcb8d511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Runtime   : \tLocal\n",
      "üíª Framework : \ttorch\n",
      "üñ•Ô∏è Device    : \tmps (Apple Metal (M1/M2))\n",
      "üî¢ Count     : \t1 \n",
      "\n",
      "... happy coding! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Device detection --\n",
    "DEVICE = \"cpu\"\n",
    "DEVICE_INFO = {\n",
    "    \"runtime\": runtime_env,\n",
    "    \"framework\": None,\n",
    "    \"name\": None,\n",
    "    \"count\": 0,\n",
    "}\n",
    "\n",
    "# Try PyTorch first\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = \"cuda\"\n",
    "        DEVICE_INFO.update({\n",
    "            \"framework\": \"torch\",\n",
    "            \"name\": torch.cuda.get_device_name(0),\n",
    "            \"count\": torch.cuda.device_count(),\n",
    "        })\n",
    "    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        DEVICE = \"mps\"\n",
    "        DEVICE_INFO.update({\n",
    "            \"framework\": \"torch\",\n",
    "            \"name\": \"Apple Metal (M1/M2)\",\n",
    "            \"count\": 1,\n",
    "        })\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# If still CPU, try TensorFlow\n",
    "if DEVICE == \"cpu\":\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "        if gpus:\n",
    "            DEVICE = \"gpu\"\n",
    "            DEVICE_INFO.update({\n",
    "                \"framework\": \"tensorflow\",\n",
    "                \"name\": gpus[0].device_type,\n",
    "                \"count\": len(gpus),\n",
    "            })\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# --- Log the result ---\n",
    "print(f\"üîé Runtime   : \\t{DEVICE_INFO['runtime']}\")\n",
    "print(f\"üíª Framework : \\t{DEVICE_INFO['framework'] or 'none'}\")\n",
    "print(f\"üñ•Ô∏è Device    : \\t{DEVICE} ({DEVICE_INFO['name'] or 'generic CPU'})\")\n",
    "print(f\"üî¢ Count     : \\t{DEVICE_INFO['count']} \\n\")\n",
    "print(f\"... happy coding! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ac71b-c4c8-4512-a084-f8f8f74b2499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a27a44a8-5ffc-4d45-b04f-50f5a494bfed",
   "metadata": {},
   "source": [
    "## c. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2a976ec-230c-4768-af50-6f22befd6106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Local environment detected ‚Äî skipping pip install.\n",
      "Use Poetry/venv to manage dependencies (requirements.txt is for reproducibility).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Dependency install (only in Colab) ---\n",
    "if IN_COLAB:\n",
    "    required_pkgs = [\n",
    "        \"docling==2.70.0\",\n",
    "        \"chromadb==1.4.1\",\n",
    "        \"langchain==1.2.7\",\n",
    "        \"langchain-core==1.2.7\",\n",
    "        \"langchain-community==0.4.1\",\n",
    "        \"langchain-text-splitters==1.1.0\",\n",
    "        \"langchain-ollama==1.0.1\",\n",
    "        \"ollama==0.6.1\",\n",
    "        \"requests==2.32.5\",\n",
    "        \"pandas==2.3.3\",\n",
    "        \"tqdm==4.67.1\",\n",
    "    ]\n",
    "\n",
    "    # Install quietly but show errors if anything fails\n",
    "    print(\"\\nInstalling core dependencies for Colab...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *required_pkgs])\n",
    "\n",
    "    print(\"Core dependencies installed.\")\n",
    "\n",
    "    # NOTE:\n",
    "    # Ollama server is not typically available in Colab by default.\n",
    "    # If your notebook uses ChatOllama/OllamaEmbeddings, you‚Äôll need a reachable Ollama endpoint.\n",
    "    # For Colab demos, consider switching to a hosted model provider or a local embedding model.\n",
    "    \n",
    "else:\n",
    "    print(\"\\nLocal environment detected ‚Äî skipping pip install.\")\n",
    "    print(\"Use Poetry/venv to manage dependencies (requirements.txt is for reproducibility).\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ad4f613c-8532-4b60-9746-0cd9bcf988c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d3e7fcf-70a6-4205-b53b-d11fd845de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, Tuple, List, Optional, Literal, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d830993c-030a-4e81-9c32-e59c723a42ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "import textwrap\n",
    "\n",
    "# LanchChain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff21b26-820b-4737-bc99-328242c2f0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d47e4ce5-7538-4157-81bd-2473d5ae6398",
   "metadata": {},
   "source": [
    "# 1. Ingesting and Indexing Data\n",
    "\n",
    "**Workflow Overview**\n",
    "\n",
    "This section describes the ingestion and indexing workflow, where raw PDF documents are converted into an index-ready representation for downstream retrieval, metadata extraction, and RAG-based question answering.\n",
    "\n",
    "The goal at this stage is grounding, not interpretation. Documents are normalized into text with stable identifiers and provenance, regardless of whether they are digitally generated PDFs or scanned reports requiring OCR. Each document is processed independently to ensure robustness, traceability, and reproducibility.\n",
    "\n",
    "The output of this step is a collection of documents that:\n",
    "\n",
    "- have a deterministic `document ID`\n",
    "\n",
    "- expose a canonical text representation\n",
    "\n",
    "- retain source-level metadata\n",
    "\n",
    "- are ready for chunking, embedding, and vector indexing\n",
    "\n",
    "This creates a clean boundary between data ingestion and semantic reasoning, allowing later stages of the pipeline to remain decoupled from document format and extraction details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fbc5ca3-78d9-4405-a1de-0b25086c8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper functions\n",
    "import pdf_ingestor.ingest as ingest\n",
    "import pdf_ingestor.retrieval as retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1e2eaa7-6de4-4586-8832-8f77ead66352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running!\n"
     ]
    }
   ],
   "source": [
    "retrieval.assert_ollama_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebb2e4-7159-4955-b26c-93b578c065dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b23a62f-0c9c-4b3c-8686-f7d2bcc9d827",
   "metadata": {},
   "source": [
    "## Step 1.1 ‚Äî Document Discovery & Normalisation\n",
    "\n",
    "We begin by discovering PDF files under a dataset directory and establishing stable identity and provenance.\n",
    "\n",
    "Key outputs:\n",
    "- a batch of `LoadedDocument` objects\n",
    "- stable `doc_id` per file (hash-based)\n",
    "- path-derived metadata (e.g., `year`)\n",
    "- ingestion summary (success/failure) without stopping the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38304978-b61e-4ac3-abf5-d4ceca561f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /Users/pepetavo/Codes/Projects/datascience/portfolio/genai/pdf_ingestor/data/WEF/2026\n",
      " - .DS_Store\n",
      " - WEF_A_New_Era_for_Digital_Health_2026.pdf\n",
      " - WEF_Chief_Economists_Outlook_January_2026.pdf\n",
      " - WEF_From_Blueprint_to_Reality_2026.pdf\n",
      " - WEF_Fuelling_the_Future_2026.pdf\n",
      " - WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [02:50<00:00, 34.16s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion summary\n",
      "Loaded 5 documents\n",
      "‚úì Successful: 5\n",
      "‚úó Failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset subset to ingest (keeps notebook runs fast and focused)\n",
    "DATASET_DIR = DATA_DIR / \"WEF\" / \"2026\"\n",
    "\n",
    "# Show a quick preview of what's inside the dataset folder (first ~20 items)\n",
    "print(\"Dataset path:\", DATASET_DIR)\n",
    "for p in sorted(DATASET_DIR.iterdir())[:20]:\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "# Ingest PDFs recursively from the dataset directory\n",
    "documents = ingest.load_files_from_path(str(DATASET_DIR))\n",
    "\n",
    "# Summarize ingestion outcome\n",
    "total = len(documents)\n",
    "successful = sum(doc.ok for doc in documents)\n",
    "failed = total - successful\n",
    "\n",
    "print(\"\\nIngestion summary\")\n",
    "print(f\"Loaded {total} documents\")\n",
    "print(f\"‚úì Successful: {successful}\")\n",
    "print(f\"‚úó Failed: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84dac47a-6360-46d7-b96d-b679bfdb807b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No failed documents [-Ok!-]\n"
     ]
    }
   ],
   "source": [
    "# Print failure reasons (if any) to debug ingestion reliability\n",
    "failed_docs = [d for d in documents if not d.ok]\n",
    "\n",
    "if failed_docs:\n",
    "    print(\"Failed documents (showing up to 10):\")\n",
    "    for d in failed_docs[:10]:\n",
    "        print(f\"\\n[FAILED] {Path(d.source_path).name}\")\n",
    "        print(\"Path:\", d.source_path)\n",
    "        print(\"Status:\", d.status)\n",
    "        print(\"Reason:\", d.error)\n",
    "else:\n",
    "    print(\"No failed documents [-Ok!-]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fac19f-9301-40f7-99a5-71232d2dd5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee42d40-2617-452c-80bf-7c707fc3e594",
   "metadata": {},
   "source": [
    "## Step 1.2 ‚Äî Content Extraction via Docling\n",
    "\n",
    "Each PDF is converted via Docling into a structured document representation and exported into a canonical text form (Markdown).\n",
    "\n",
    "In this notebook, we validate extraction quality by:\n",
    "- printing filename + metadata (e.g., year)\n",
    "- showing a short text preview\n",
    "- checking extracted character lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9075c77e-d5b5-4dc4-bbb4-125f93e5c97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful documents: 5\n",
      "\n",
      "üìÑ WEF_A_New_Era_for_Digital_Health_2026.pdf\n",
      " - year: 2026\n",
      " - extracted chars: 98326\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In collaboration with Department of Health - Abu Dhabi  ## A New Era for Digital Health: Abu\n",
      "Dhabi's Leap to Health Intelligence  WHI T E   P A P E R J A N U A R Y   2 0 2 6  <!-- image\n",
      "-->  ## Contents  | Forewords\n",
      "\n",
      "üìÑ WEF_From_Blueprint_to_Reality_2026.pdf\n",
      " - year: 2026\n",
      " - extracted chars: 122383\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In collaboration with Oliver Wyman  ## From Blueprint to Reality: A Stronger Business Case for\n",
      "Shared Energy Infrastructure  L E A R N I N G S   F R O M   T R A N S I T I O N I N G I N D U S\n",
      "T R I A L   C L U S T E R S  WHI T E   P A P E R J A N U A R Y   2 0 2 6  <!-- image -->  ##\n",
      "Contents  | Foreword          | Foreword\n",
      "| Foreword                                                                    |   3 |\n",
      "|-------------------|-\n",
      "\n",
      "üìÑ WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf\n",
      " - year: 2026\n",
      " - extracted chars: 224851\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<!-- image -->  In collaboration with¬†Accenture  ## Industrial Transformation in ASEAN: A\n",
      "Cluster-Driven Model for Regional and Global Collaboration  WHI T E   P A P E R J A N U A R Y\n",
      "2 0 2 6  <!-- image -->  <!-- image -->  ## Contents  | Foreword\n",
      "| Foreword                                                                             |\n",
      "Foreword\n"
     ]
    }
   ],
   "source": [
    "# Preview a few successfully extracted documents\n",
    "ok_docs = [d for d in documents if d.ok]\n",
    "\n",
    "print(f\"Successful documents: {len(ok_docs)}\")\n",
    "\n",
    "for doc in ok_docs[:3]:\n",
    "    # Extract a readable filename (avoid long absolute path)\n",
    "    filename = Path(doc.source_path).name\n",
    "\n",
    "    # Optional-safe access to extracted content\n",
    "    content: Optional[str] = doc.content\n",
    "    if content is None or not content.strip():\n",
    "        preview = \"[Empty content]\"\n",
    "        content_len = 0\n",
    "    else:\n",
    "        content_len = len(content)\n",
    "        preview = textwrap.fill(content[:500].replace(\"\\n\", \" \"), width=95)\n",
    "\n",
    "    # Optional metadata (path-derived)\n",
    "    year = doc.metadata.get(\"year\")\n",
    "\n",
    "    print(f\"\\nüìÑ {filename}\")\n",
    "    print(f\" - year: {year if year is not None else 'Unknown'}\")\n",
    "    print(f\" - extracted chars: {content_len}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baffe5e9-c581-448d-9b7e-6a82860876a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6daa3e9-6291-40c0-8792-fe0869be1c60",
   "metadata": {},
   "source": [
    "## Step 1.3 ‚Äî Wrapper Abstraction (`LoadedDocument`)\n",
    "\n",
    "We wrap Docling outputs into a stable `LoadedDocument` interface so downstream code does not depend on Docling internals.\n",
    "\n",
    "Benefits:\n",
    "- consistent `.ok` contract for batch pipelines\n",
    "- stable `.text()` interface for indexing\n",
    "- chunk generator `.iter_chunks()` with provenance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bae795a-be83-4763-9bf4-35e58a21edf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapper inspection\n",
      "doc_id: 20b09c78e25cc45d\n",
      "source: /Users/pepetavo/Codes/Projects/datascience/portfolio/genai/pdf_ingestor/data/WEF/2026/WEF_A_New_Era_for_Digital_Health_2026.pdf\n",
      "ok: True\n",
      "metadata keys: ['doc_id', 'source_path', 'year', 'ocr_performed', 'processing_notes', 'conversion_status']\n",
      "\n",
      "Canonical text preview (first 300 chars):\n",
      "In collaboration with Department of Health - Abu Dhabi  ## A New Era for Digital Health: Abu Dhabi's Leap to Health Intelligence  WHI T E   P A P E R J A N U A R Y   2 0 2 6  <!-- image -->  ## Contents  | Forewords                                                                                     \n"
     ]
    }
   ],
   "source": [
    "# Inspect the wrapper object for one successful document\n",
    "if ok_docs:\n",
    "    sample_doc = ok_docs[0]\n",
    "\n",
    "    print(\"Wrapper inspection\")\n",
    "    print(\"doc_id:\", sample_doc.doc_id)\n",
    "    print(\"source:\", sample_doc.source_path)\n",
    "    print(\"ok:\", sample_doc.ok)\n",
    "    print(\"metadata keys:\", list(sample_doc.metadata.keys()))\n",
    "\n",
    "    # Canonical text view (used for chunking/indexing)\n",
    "    canonical_text = sample_doc.text()\n",
    "    print(\"\\nCanonical text preview (first 300 chars):\")\n",
    "    print(canonical_text[:300].replace(\"\\n\", \" \"))\n",
    "else:\n",
    "    print(\"No successful documents to inspect.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec5485-0a6a-483b-b111-3bf7cc11cb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "999c9822-0fdc-469b-9253-cca36d058654",
   "metadata": {},
   "source": [
    "## Step 1.4 ‚Äî Converting to LangChain Documents (LangChain Adapter)\n",
    "\n",
    "At this stage, PDFs have already been ingested and normalized into a framework-agnostic `LoadedDocument` representation.\n",
    "\n",
    "To use the LangChain ecosystem (splitters, vector stores, retrievers), we now convert each successful `LoadedDocument` into a LangChain `Document`.\n",
    "\n",
    "This step is intentionally performed at the notebook level to:\n",
    "- keep ingestion independent of orchestration frameworks\n",
    "- make framework choices explicit and auditable\n",
    "- enable side-by-side comparison between custom chunking and LangChain-based pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fc11c2b-8987-42d7-b12b-79606da9ab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 5 documents to LangChain format.\n"
     ]
    }
   ],
   "source": [
    "# Container for LangChain documents (each represents one ingested PDF as a \"document\")\n",
    "lc_documents: List[Document] = []\n",
    "\n",
    "# Iterate over ingestion outputs (LoadedDocument wrappers)\n",
    "for doc in documents:\n",
    "    # Skip documents that failed ingestion/conversion\n",
    "    if not doc.ok:\n",
    "        continue\n",
    "\n",
    "    # Optional-safe: extracted content should be a non-empty string\n",
    "    # (In our pipeline this is typically Markdown exported from Docling)\n",
    "    text = doc.content\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        continue\n",
    "\n",
    "    # Convert to a LangChain Document with preserved provenance metadata\n",
    "    lc_documents.append(\n",
    "        Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                # Stable ID for de-duplication / traceability\n",
    "                \"doc_id\": doc.doc_id,\n",
    "                # Full path allows full provenance trace-back\n",
    "                \"source_path\": doc.source_path,\n",
    "                # Filename is convenient for notebooks and UI\n",
    "                \"filename\": Path(doc.source_path).name,\n",
    "                # Optional metadata (derived from folder structure in your loader)\n",
    "                \"year\": doc.metadata.get(\"year\"),\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"Converted {len(lc_documents)} documents to LangChain format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8b679-8e03-48ce-a0d9-b401b7294909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6be06aa-5426-4a1a-88b8-40bb2e862cdd",
   "metadata": {},
   "source": [
    "## Step 1.5 ‚Äî Chunk Preparation for Indexing (LangChain Splitter)\n",
    "\n",
    "Vector search operates on **chunks**, not full documents.  \n",
    "We therefore split each LangChain `Document` into overlapping text chunks.\n",
    "\n",
    "Design goals:\n",
    "- deterministic chunking (reproducible results)\n",
    "- overlap to preserve context across chunk boundaries\n",
    "- provenance preserved through LangChain `metadata` propagation\n",
    "- chunk size tuned for embedding + retrieval (not for summarization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21b66638-5ed2-453d-beaa-f8d998cd29dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 817\n",
      "Example chunk metadata: {'doc_id': '20b09c78e25cc45d', 'source_path': '/Users/pepetavo/Codes/Projects/datascience/portfolio/genai/pdf_ingestor/data/WEF/2026/WEF_A_New_Era_for_Digital_Health_2026.pdf', 'filename': 'WEF_A_New_Era_for_Digital_Health_2026.pdf', 'year': 2026}\n",
      "Example chunk preview: In collaboration with Department of Health - Abu Dhabi  ## A New Era for Digital Health: Abu Dhabi's Leap to Health Intelligence  WHI T E   P A P E R J A N U A R Y   2 0 2 6  <!-- image -->  ## Contents\n"
     ]
    }
   ],
   "source": [
    "# Define a deterministic chunking strategy\n",
    "# - separators define preferred breakpoints (paragraphs ‚Üí lines ‚Üí spaces ‚Üí characters)\n",
    "# - overlap preserves context across boundaries\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Split LangChain documents into chunk-level LangChain Documents\n",
    "lc_chunks = splitter.split_documents(lc_documents)\n",
    "\n",
    "# Basic sanity checks to confirm chunking worked\n",
    "print(\"Chunks:\", len(lc_chunks))\n",
    "\n",
    "# Optional-safe: only print example metadata if at least one chunk exists\n",
    "if lc_chunks:\n",
    "    print(\"Example chunk metadata:\", lc_chunks[0].metadata)\n",
    "    print(\"Example chunk preview:\", lc_chunks[0].page_content[:250].replace(\"\\n\", \" \"))\n",
    "else:\n",
    "    print(\"No chunks produced (check upstream extraction).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedb3dc-60fa-4578-bf28-83bbfd7e61c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3360e5b1-daf5-4a1a-a4d0-9252d68fb293",
   "metadata": {},
   "source": [
    "## Step 1.6 ‚Äî Index-Ready Output (Handoff Boundary)\n",
    "\n",
    "At this stage the pipeline has produced a **normalized, index-ready representation** of the document corpus:\n",
    "\n",
    "- per-document text in LangChain `Document` format (`lc_documents`)\n",
    "- per-chunk text units (`lc_chunks`) with stable provenance and metadata\n",
    "  (doc_id, filename, source_path, year, etc.)\n",
    "\n",
    "No embeddings or vector indexing are performed yet. \n",
    "\n",
    "This step is a **clean handoff boundary** between document preparation and semantic processing. Downstream stages can now safely perform:\n",
    "\n",
    "- embedding generation\n",
    "- vector database construction (e.g., Chroma)\n",
    "- retrieval validation\n",
    "- metadata extraction and RAG-based question answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9fc68-b00b-4c0c-a06f-3fc2ec4cb6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5da5a580-ec2d-4ef4-9fa7-1c1161de4121",
   "metadata": {},
   "source": [
    "## Step 1.7 ‚Äî Embedding Model (Local)\n",
    "\n",
    "We now select an embedding model to convert each chunk into a vector representation.\n",
    "\n",
    "In this project we use **local embeddings via Ollama** to keep the pipeline:\n",
    "- offline-friendly\n",
    "- reproducible\n",
    "- aligned with a local-first deployment scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "363ff384-1925-4cb2-adc1-87342efbf4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings initialized: base_url='http://localhost:11434' model='nomic-embed-text' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None\n"
     ]
    }
   ],
   "source": [
    "# Create an embeddings client backed by Ollama (local)\n",
    "# Ensure the embedding model is available in Ollama (e.g., `nomic-embed-text`)\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "print(\"Embeddings initialized:\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca80c3-555b-4bd6-8282-968bcfe4e2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60fb2f15-3711-4d61-b53b-fad2ee30741e",
   "metadata": {},
   "source": [
    "## Step 1.8 ‚Äî Vector Database (Chroma)\n",
    "\n",
    "We build a local Chroma vector database from the chunked documents.\n",
    "\n",
    "What this stores:\n",
    "- chunk embeddings (vectors)\n",
    "- the chunk text (`page_content`)\n",
    "- metadata for provenance and filtering (year, filename, doc_id, etc.)\n",
    "\n",
    "We also persist the database to disk so it can be reused without re-embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9114e441-1b57-4b2e-8b5f-e9747bab73e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma saved to: /Users/pepetavo/Codes/Projects/datascience/portfolio/genai/pdf_ingestor/data/index/chroma_wef\n",
      "Collection: wef_reports\n",
      "Chunks indexed: 817\n"
     ]
    }
   ],
   "source": [
    "# Define where Chroma should persist on disk (local-first)\n",
    "CHROMA_DIR = f\"{DATA_DIR}/index/chroma_wef\"\n",
    "\n",
    "# Build the vector store from chunk documents\n",
    "# - Chroma will embed each chunk using the embeddings model\n",
    "# - metadata is stored alongside vectors for filtering and traceability\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=lc_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=CHROMA_DIR,\n",
    "    collection_name=\"wef_reports\",\n",
    ")\n",
    "\n",
    "# Persist to disk to avoid rebuilding the index in future runs\n",
    "vectordb.persist()\n",
    "\n",
    "print(\"Chroma saved to:\", CHROMA_DIR)\n",
    "print(\"Collection:\", \"wef_reports\")\n",
    "print(\"Chunks indexed:\", len(lc_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c92c3-3679-45f6-a3da-e19336bbb7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec37e96d-def5-43b9-99e0-0919df0e3685",
   "metadata": {},
   "source": [
    "# 2. Retrieve Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5901418-0230-447a-a340-2090061a1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever libraries\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_classic.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# chain composition\n",
    "from langchain_classic.schema.runnable import RunnablePassthrough\n",
    "from langchain_classic.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e06fea-b4a9-4b87-8296-f62cd155216e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "937c4b33-8afe-445c-9e2e-2261953bdbe7",
   "metadata": {},
   "source": [
    "## Step 2.1 ‚Äî Language Model Setup (Local LLM)\n",
    "\n",
    "In this section we configure the local Large Language Model (LLM) used for:\n",
    "- query rewriting (to improve retrieval recall)\n",
    "- answer generation in the RAG pipeline\n",
    "\n",
    "The model is served locally via **Ollama**, keeping the workflow:\n",
    "- offline-friendly\n",
    "- reproducible\n",
    "- suitable for on-prem or air-gapped environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a4e3463d-4eed-43ad-ab64-cfbb2233d29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ollama model 'llama3.2' resolved to 'llama3.2:latest'.\n",
      "LLM initialized successfully: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# Define the local LLM model to use\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "\n",
    "# Preflight check: ensure Ollama + model are available\n",
    "retrieval.assert_ollama_model_available(MODEL_LLAMA)\n",
    "\n",
    "# Instantiate the chat-based LLM\n",
    "llm = ChatOllama(\n",
    "    model=MODEL_LLAMA,\n",
    "    temperature=0.0,  # deterministic output for retrieval + QA\n",
    ")\n",
    "\n",
    "print(\"LLM initialized successfully:\", MODEL_LLAMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ee8f1-bdd9-41c6-808c-51b105a2de7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "279ded5e-bc8c-4ea4-9d57-0f258f61a33f",
   "metadata": {},
   "source": [
    "## Step 2.2 ‚Äî Query Rewriting for Improved Retrieval\n",
    "\n",
    "Vector similarity search can miss relevant documents if the query wording\n",
    "does not align with the indexed text.\n",
    "\n",
    "To mitigate this, we use **Multi-Query Retrieval**, where the LLM generates\n",
    "multiple alternative phrasings of the user question.\n",
    "\n",
    "These alternative queries are used to retrieve a broader and more diverse\n",
    "set of relevant chunks from the vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff2a89ee-b27c-49d5-ac9a-6c1c8e12ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-query prompt ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prompt that instructs the LLM to generate alternative versions of a query\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant tasked with improving document retrieval.\n",
    "\n",
    "Generate 3 alternative versions of the user's question that capture\n",
    "different perspectives or phrasings, while preserving the original intent.\n",
    "\n",
    "Return each alternative on a new line.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "print(\"Multi-query prompt ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb749e-a292-4ce9-acb8-e15948dfada7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3140ddb-d61c-4e9b-acf8-b96975fc2afe",
   "metadata": {},
   "source": [
    "## Step 2.3 ‚Äî Retriever Configuration\n",
    "\n",
    "We now configure the retriever that connects the vector database to the LLM.\n",
    "\n",
    "The **MultiQueryRetriever**:\n",
    "- takes a user question\n",
    "- generates alternative queries using the LLM\n",
    "- retrieves relevant chunks for each query\n",
    "- merges and deduplicates the results\n",
    "\n",
    "This improves recall compared to single-query similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a301d71-9508-499c-8165-3ba03c49e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever configured (MultiQueryRetriever).\n"
     ]
    }
   ],
   "source": [
    "# Convert Chroma vector store into a retriever interface\n",
    "base_retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 5}  # number of chunks per query\n",
    ")\n",
    "\n",
    "# Wrap the base retriever with multi-query logic\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever,\n",
    "    llm=llm,\n",
    "    prompt=QUERY_PROMPT,\n",
    ")\n",
    "\n",
    "print(\"Retriever configured (MultiQueryRetriever).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ff9c3-f498-409c-8775-378770e9f2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64292ee7-9daf-4a3d-bd4a-4c9bba1cc164",
   "metadata": {},
   "source": [
    "## Step 2.4 ‚Äî RAG Prompt Template\n",
    "\n",
    "The RAG prompt constrains the LLM to:\n",
    "- answer **only** using retrieved context\n",
    "- avoid hallucinations\n",
    "- ground responses in indexed documents\n",
    "\n",
    "This is critical for trust, traceability, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05f128cd-e71c-46cc-90dc-8f0603933101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG prompt template ready.\n"
     ]
    }
   ],
   "source": [
    "# Prompt template for RAG-based answering\n",
    "rag_template = \"\"\"\n",
    "Answer the question using ONLY the context provided below.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If the answer cannot be found in the context, say so explicitly.\n",
    "\"\"\"\n",
    "\n",
    "# Convert to a chat-compatible prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "print(\"RAG prompt template ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd9284-7011-40c1-bc44-e6711087a6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e3b6983-b4ba-4184-abb8-0ba93f89ab28",
   "metadata": {},
   "source": [
    "## Step 2.5 ‚Äî Chain Composition\n",
    "\n",
    "We now assemble the full Retrieval-Augmented Generation (RAG) chain.\n",
    "\n",
    "The chain performs:\n",
    "1. retrieval of relevant chunks (`retriever`)\n",
    "2. injection of retrieved context into the prompt\n",
    "3. answer generation via the LLM\n",
    "4. output parsing into a clean string\n",
    "\n",
    "This composition uses LangChain's Runnable interface for clarity and modularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "667b88bd-f57c-40cd-8b36-057a744b4f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain assembled.\n"
     ]
    }
   ],
   "source": [
    "# Compose the RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        # Retrieved context comes from the retriever\n",
    "        \"context\": retriever,\n",
    "        # The original question is passed through unchanged\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | rag_prompt   # Inject context + question into prompt\n",
    "    | llm          # Generate answer using local LLM\n",
    "    | StrOutputParser()  # Parse output to plain string\n",
    ")\n",
    "\n",
    "print(\"RAG chain assembled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c13e6d-3b86-49f7-8e65-0942d92423bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57a25339-b184-46ad-a085-254e76085771",
   "metadata": {},
   "source": [
    "# 3. Generate Responses\n",
    "\n",
    "Finally, we execute the RAG pipeline with a sample question and inspect\n",
    "the generated answer.\n",
    "\n",
    "This step validates:\n",
    "- retrieval quality\n",
    "- grounding in document context\n",
    "- end-to-end pipeline integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "12190fc1-789a-44a5-b3f0-61d0e657729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to wrap up discussion\n",
    "\n",
    "def ask_me_about(question: str):\n",
    "    \"\"\"\n",
    "    Chat with the agent using the RAG chain\n",
    "    \"\"\"\n",
    "    return display(Markdown(rag_chain.invoke(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "937c987c-cf97-4e37-af20-a5970bbf868f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, the main themes in the future of energy discussed in these reports are:\n",
       "\n",
       "1. **Clean Fuels**: The reports emphasize the importance of clean fuels in reducing emissions and diversifying energy supply. They discuss various pathways for producing clean fuels, including liquid biofuels, biogases, lower-carbon fossil fuels, synthetic fuels, and hydrogen derivatives (WEF_Fuelling_the_Future_2026.pdf).\n",
       "2. **Energy Transition**: The reports highlight the need for a rapid transition to clean energy sources, with a focus on reducing greenhouse gas emissions and mitigating climate change. They discuss the role of clean fuels in this transition and the importance of investing in emerging technologies (WEF_From_Blueprint_to_Reality_2026.pdf).\n",
       "3. **Regional Realities**: The reports acknowledge that the role of clean fuels will differ by region, depending on local strengths, resources, and demand dynamics. They emphasize the need for a regional approach to evaluating competitiveness and guiding investment towards the most competitive and high-impact options (WEF_Fuelling_the_Future_2026.pdf).\n",
       "4. **Industrial Transformation**: The reports discuss the importance of industrial transformation in energy ecosystems as a driver of competitiveness, innovation, and job creation. They highlight the need for innovative business models that mobilize capital at scale, direct resources efficiently, and deliver projects with widespread commercial and societal value (WEF_From_Blueprint_to_Reality_2026.pdf).\n",
       "\n",
       "These themes are discussed across multiple reports, including WEF_Fuelling_the_Future_2026.pdf, WEF_A_New_Era_for_Digital_Health_2026.pdf, and WEF_From_Blueprint_to_Reality_2026.pdf.\n",
       "\n",
       "References:\n",
       "\n",
       "* WEF_Fuelling_the_Future_2026.pdf\n",
       "* WEF_A_New_Era_for_Digital_Health_2026.pdf\n",
       "* WEF_From_Blueprint_to_Reality_2026.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example query relevant to the WEF corpus\n",
    "question = \"\"\"\n",
    "    What are the main themes in the future of energy discussed in these reports?\"\n",
    "    In your response, please include the reference of the files evaluated for your\n",
    "    response\n",
    "\"\"\"\n",
    "\n",
    "ask_me_about(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "156fac0f-1948-4e54-a708-aa2042c505d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, the key strategic themes shaping the global energy transition across the 2026 World Economic Forum reports can be synthesized as follows:\n",
       "\n",
       "1. **Clean Fuels and Energy Transition**: This theme is prominent across multiple reports, including \"WEF_Fuelling_the_Future_2026.pdf\", \"WEF_From_Blueprint_to_Reality_2026.pdf\", and \"WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf\". These reports emphasize the importance of clean fuels, such as biofuels, hydrogen derivatives, and lower-carbon fossil fuels, in driving a more secure, affordable, and sustainable energy system. They also highlight the need for industrial transformation to unlock competitiveness, innovation, and job creation.\n",
       "\n",
       "2. **Industrial Transformation and Cluster Development**: This theme is explored in \"WEF_From_Blueprint_to_Reality_2026.pdf\" and \"WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf\". The reports discuss the need for innovative business models, mobilizing capital at scale, and directing resources efficiently to deliver projects that provide widespread commercial and societal value. They also highlight the importance of transitioning industrial clusters to drive competitiveness, innovation, and job creation.\n",
       "\n",
       "3. **Global Cooperation and Collaboration**: This theme is evident in \"WEF_Fuelling_the_Future_2026.pdf\" and \"WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf\". The reports emphasize the need for greater alignment, coordination, and practical mechanisms for collective action to address the global energy transition. They also highlight the importance of partnerships across the value chain, engagement with financiers early on, and adopting new investment models.\n",
       "\n",
       "4. **Sustainability and Emissions Reduction**: This theme is present in \"WEF_Fuelling_the_Future_2026.pdf\" and \"WEF_From_Blueprint_to_Reality_2026.pdf\". The reports discuss the potential of clean fuel pathways to reduce emissions by at least 50% compared to conventional fuels, with some optimal set-ups achieving up to 90%. They also highlight the importance of full lifecycle carbon intensity, feedstock type, crop yield, and agricultural practices in driving emissions.\n",
       "\n",
       "While these themes are not explicitly stated as \"key strategic themes\" in a single report, they are consistently discussed across multiple reports, providing a comprehensive understanding of the global energy transition in 2026."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cross-cutting strategic themes\n",
    "question = \"\"\"\n",
    "What are the key strategic themes shaping the global energy transition across\n",
    "the 2026 World Economic Forum reports?\n",
    "\n",
    "Please synthesize insights across the documents and explicitly reference\n",
    "which reports informed each theme in your response.\n",
    "\"\"\"\n",
    "\n",
    "ask_me_about(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f6fb20f9-9a0f-4d3f-ae77-6e61fefcbc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the 2026 World Economic Forum reports, the energy transition is linked to industrial transformation and regional competitiveness. The reports highlight that industry is the economic engine of ASEAN, the largest energy consumer among the end-use sector, and the second-largest CO2 emitter.\n",
       "\n",
       "The reports emphasize that industrial clusters play a crucial role in advancing both ASEAN's industrial transformation and overall energy transition. By examining the factors shaping ASEAN's energy transition, it becomes clear what influences progress and enables cluster development.\n",
       "\n",
       "Specifically, the reports mention that:\n",
       "\n",
       "* Industry accounts for around 47% of South-East Asia's total final energy consumption, with industrial energy demand expected to rise by 65% by 2050 from 2023.\n",
       "* Industrial clusters are key levers to advance both ASEAN's industrial transformation and overall energy transition.\n",
       "* The region's next leap requires moving from project-by-project deals to coordinated, system-level investment - where grids, industrial clusters, and supply chains work in sync to make low-carbon growth bankable.\n",
       "\n",
       "These points are mentioned in the following WEF reports:\n",
       "\n",
       "* \"Industrial Transformation in ASEAN: A Cluster-Driven Approach\" (2026)\n",
       "* \"ASEAN's Energy Transition: A Catalyst for Industrial Transformation\" (2026)\n",
       "* \"Fuelling the Future of Industry in Southeast Asia\" (2026)\n",
       "\n",
       "The reports also highlight that ASEAN's energy transition will not hinge solely on technology availability, but on the credibility and predictability of its financial and policy architecture.\n",
       "\n",
       "Sources:\n",
       "\n",
       "* World Economic Forum. (2026). Industrial Transformation in ASEAN: A Cluster-Driven Approach.\n",
       "* World Economic Forum. (2026). ASEAN's Energy Transition: A Catalyst for Industrial Transformation.\n",
       "* World Economic Forum. (2026). Fuelling the Future of Industry in Southeast Asia.\n",
       "\n",
       "Note: The exact titles and sources may vary depending on the specific report and edition."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Energy √ó Industrial transformation\n",
    "\n",
    "question = \"\"\"\n",
    "How is the energy transition linked to industrial transformation and regional\n",
    "competitiveness in the 2026 World Economic Forum reports?\n",
    "\n",
    "In your answer, explain the role of energy systems in enabling industrial\n",
    "change and cite the specific WEF reports used.\n",
    "\"\"\"\n",
    "\n",
    "ask_me_about(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "51e532b1-90c6-48f0-841d-1d68b6d1f9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, the 2026 World Economic Forum reports identify the following main economic and policy barriers to accelerating the energy transition:\n",
       "\n",
       "1. **Infrastructure barriers**: The reports highlight the need for significant investment in infrastructure to support the transition to a low-carbon economy. This includes upgrading existing infrastructure, building new green infrastructure, and developing smart grids.\n",
       "\n",
       "Document: \"ASEAN will need $11.9 trillion by 2050 to fully transition across energy sectors.\" (WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf)\n",
       "\n",
       "2. **Human capital and workforce transition**: The reports note that the education system in ASEAN countries lags behind industry needs, leading to a skills gap in the energy sector.\n",
       "\n",
       "Document: \"Human capital and workforce transition: As education systems lag behind industry needs, with limited curricula, instructor shortages and weak academia-industry-government coordination, fragmented funding continues to widen the region's energy skills gap.\" (WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf)\n",
       "\n",
       "3. **Financial barriers**: The reports highlight the need for stronger policy alignment and market coherence across member states, as well as the importance of mobilizing finance to scale up low-carbon industrial projects.\n",
       "\n",
       "Document: \"Addressing these barriers is an economic and competitiveness imperative. Investment will ultimately follow returns, so scaling-up low-carbon industrial projects depends on providing credible profitability and risk-adjusted performance.\" (WEF_From_Blueprint_to_Reality_2026.pdf)\n",
       "\n",
       "4. **Policy alignment and market coherence**: The reports emphasize the need for stronger policy alignment and market coherence across member states to support the energy transition.\n",
       "\n",
       "Document: \"Achieving ASEAN's energy transition requires stronger policy alignment and market coherence across member states. Harmonizing technical and market standards such as grid codes, tariff structures and REC mechanisms while gradually rebalancing fossil fuel incentives and developing a common carbon pricing and disclosure framework will strengthen investor confidence.\" (WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf)\n",
       "\n",
       "To bridge the gap between strategy and implementation, the reports suggest:\n",
       "\n",
       "1. **Building collaborative ecosystems**: The reports propose building industrial clusters as collaborative ecosystems that can test and demonstrate clean technologies, reduce risks, and enhance project bankability.\n",
       "\n",
       "Document: \"Industrial clusters can serve as collaborative ecosystems and testbeds for clean technologies like renewables, hydrogen and shared carbon capture networks reducing risks and cost.\" (WEF_Industrial_Transformation_in_ASEAN_A_Cluster-Driven_Model_for_Regional_and_Global_Collaboration_2026.pdf)\n",
       "\n",
       "2. **Mobilizing finance**: The reports suggest mobilizing finance to scale up low-carbon industrial projects, including through innovative financing mechanisms such as the CFIL (Climate Finance Innovation Lab).\n",
       "\n",
       "Document: \"Well-designed financial mechanisms connect global capital with local opportunities. Clusters provide the proof-of-concept platforms financiers need to translate roadmaps into investable projects.\" (SARABURI SANDB@X)\n",
       "\n",
       "3. **Institutionalizing climate finance innovation**: The reports propose institutionalizing climate finance innovation in Malaysia through the CFIL, which can help bridge the gap between strategy and implementation.\n",
       "\n",
       "Document: \"Bank Negara Malaysia: institutionalizing climate finance innovation in Malaysia\" (CASE STUDY 9)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Policy, economics, and execution gap\n",
    "\n",
    "question = \"\"\"\n",
    "According to the 2026 World Economic Forum reports, what are the main economic\n",
    "and policy barriers to accelerating the energy transition, and how do these\n",
    "reports suggest bridging the gap between strategy and implementation?\n",
    "\n",
    "Please reference the specific documents that support each point.\n",
    "\"\"\"\n",
    "\n",
    "ask_me_about(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ac596-0959-4b66-a6de-393496bf6d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd879c0c-c379-4c5e-a431-65e3fe896107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20a3cd28-172b-4e6e-a3a5-63327c53b50c",
   "metadata": {},
   "source": [
    "# End of session\n",
    "\n",
    "The code below is meant to clean up the database when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "24990dd2-070b-4712-820d-2e7ab0d3123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector db successfully deleted!\n"
     ]
    }
   ],
   "source": [
    "vectordb.delete_collection()\n",
    "print(\"Vector db successfully deleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18494c7c-d66b-49d6-9ad2-a08e154fd148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python GenAI (env)",
   "language": "python",
   "name": "venvgenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
